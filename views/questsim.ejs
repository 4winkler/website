<% include ./partials/header %> 

<div style="padding-top: 100px; padding-bottom: 50px;">
<div class="container">


    <h3 style="text-align: center">
      QuestSim: Human Motion Tracking from sparse Sensors using Simulated Avatars
    </h3> 

    <h6 style="text-align: center; margin-top: 1.0em">
    <div class="names-container">
      <span class="name"><a href="https://alex-winkler.com/">Alexander Winkler</a></span>
      <span class="name"><a href="https://sites.google.com/view/jungdam">Jungdam Won</a></span>
      <span class="name"><a href="http://yutingye.info/">Yuting Ye</a></span>
    </div>
    </h6>

    <h6 style="text-align: center; margin-top: 1.0em">
      Siggraph Asia 2022
    </h6> 

    <h6 style="text-align: center; margin-top: 1.0em">
      <a href="https://arxiv.org/abs/2209.09391">[PDF]</a>
    </h6> 


    <div class="container" style="margin-top: 4.0em">
      <div class="row">
        <div class="col-12 col-md-6 text-center">
          <div class="embed-responsive embed-responsive-16by9 my-youtube shadow">
            <iframe
              width="560"
              height="315"
              src="https://www.youtube.com/embed/CkTHsz6Ldas?controls=0"
              title="YouTube video player"
              frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              allowfullscreen
            ></iframe>
          </div>
        </div>
    
        <div class="col-12 col-md-6 text-center">
          <div class="embed-responsive embed-responsive-16by9 my-youtube shadow">
            <iframe
              width="560"
              height="315"
              src="https://www.youtube.com/embed/DmjsNnBhWdA?controls=0"
              title="YouTube video player"
              frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              allowfullscreen
            ></iframe>
          </div>
        </div>
      </div>
    </div>
    






    <div style="margin-top: 4.0em">
      <h4 style="text-align: center">
        Abstract
      </h4> 
      <p>
        Real-time tracking of human body motion is crucial for interactive and immersive experiences in AR/VR. However, very limited sensor data about the body is available from standalone wearable devices such as HMDs (Head Mounted Devices) or AR glasses. In this work, we present a reinforcement learning framework that takes in sparse signals from an HMD and two controllers, and simulates plausible and physically valid full body motions. Using high quality full body motion as dense supervision during training, a simple policy network can learn to output appropriate torques for the character to balance, walk, and jog, while closely following the input signals. Our results demonstrate surprisingly similar leg motions to ground truth without any observations of the lower body, even when the input is only the 6D transformations of the HMD. We also show that a single policy can be robust to diverse locomotion styles, different body sizes, and novel environments.
      </p>

      <div class="col-12 col-md-8 m-0 p-0 mx-auto text-center"">
        <img src="/images/quest_sim_overview.png">
      </div>
    </div>



    <div style="margin-top: 5.0em">
      <h4 style="text-align: center">
        Bibtex
      </h4> 
      <div class="bibtex-container">
        <pre class="bibtex-box">
@inproceedings{10.1145/3550469.3555411,
      author    = {Winkler, Alexander and Won, Jungdam and Ye, Yuting},
      title     = {QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars},
      year      = {2022},
      doi       = {10.1145/3550469.3555411},
      booktitle = {SIGGRAPH Asia 2022 Conference Papers},
      location  = {Daegu, Republic of Korea},
}
</pre>
      </div>
    </div>


</div>
</div>


</body>
</html>

<% include ./partials/footer%>


